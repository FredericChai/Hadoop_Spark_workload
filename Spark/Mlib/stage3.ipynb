{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612edd3960d64d7bbb94bd9f1e690833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1558344694965_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-7-118.ap-southeast-2.compute.internal:20888/proxy/application_1558344694965_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-11-31.ap-southeast-2.compute.internal:8042/node/containerlogs/container_1558344694965_0001_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,mean\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.feature import PCA,VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors,VectorUDT,DenseVector\n",
    "from pyspark.sql.types import DoubleType,ArrayType,IntegerType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979264682491447495497680ae937673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.executor.memory', '6g') \\\n",
    "    .config('spark.executor.cores', '3') \\\n",
    "    .config('spark.num.executor', '20') \\\n",
    "    .appName(\"Spark Text Encoder example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0555f4d5d4ae4d4f854acd924ca7d79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rev_data = \"s3://amazon-reviews-pds/tsv/amazon_reviews_us_Music_v1_00.tsv.gz\"\n",
    "revs = spark.read.csv(rev_data,header=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e63efd5b03434ebf028218c644396a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define udf for returning the list of the sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = lambda paragraph: tokenizer.tokenize(paragraph)\n",
    "udf_sentences_list = udf(sentences,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a71ed0510a0496f95ae4e85b077935e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select one product from the top 10 products list, and compute the list of sentences and stars\n",
    "top_reviews = revs.filter((revs.product_id == 'B00006J6VG') & (revs.review_body.isNotNull())) \\\n",
    "    .withColumn(\"stars\", revs.star_rating.cast(IntegerType())) \\\n",
    "    .select('review_body','stars','review_id') \\\n",
    "    .withColumn(\"sentences\", udf_sentences_list(revs.review_body)) \\\n",
    "    .drop('review_body') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b07363a27249de9375141e7173f040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def review_embed(rev_partition):\n",
    "    '''\n",
    "    embedding the sentences to 512-d vectors by tensorflow framework\n",
    "    '''\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "    embed = hub.Module(module_url)\n",
    "    ids = []\n",
    "    sen = []\n",
    "    for item in rev_partition:\n",
    "        ids.append(item[0])\n",
    "        sen.append(item[1])\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        embedding_sen = session.run(embed(sen))\n",
    "    return list(zip(ids,embedding_sen.tolist(),sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8131b15bb341998ebba293519b64cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define positive class (stars >= 4) and negative class (star <= 2)\n",
    "positive_class = top_reviews.filter(\"stars >= 4\") \\\n",
    "                .drop('stars') \\\n",
    "                .rdd.map(lambda row : [row.review_id, row.sentences]) \\\n",
    "                .flatMap(lambda item : [[item[0], item[1][i]] for i in range(len(item[1]))]) \\\n",
    "                .mapPartitions(review_embed) \\\n",
    "                .cache()\n",
    "negative_class = top_reviews.filter(\"stars <= 2\") \\\n",
    "                .drop('stars') \\\n",
    "                .rdd.map(lambda row : [row.review_id, row.sentences]) \\\n",
    "                .flatMap(lambda item : [[item[0], item[1][i]] for i in range(len(item[1]))]) \\\n",
    "                .mapPartitions(review_embed) \\\n",
    "                .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492afe897f0746e0baeb9216d2e4a89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#add index column\n",
    "positive_class_df = spark.createDataFrame(positive_class,['review_id','array','sentence'])\\\n",
    "                    .withColumn('index',f.monotonically_increasing_id()) \\\n",
    "                    .cache()         \n",
    "negative_class_df = spark.createDataFrame(negative_class,['review_id','array','sentence']) \\\n",
    "                    .withColumn('index',f.monotonically_increasing_id()) \\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866125411caf4f67b9362223e4ea7f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#transform the format of RDD from lsit to vector form\n",
    "list_to_vector_udf = udf(lambda array: Vectors.dense(array), VectorUDT())\n",
    "positive_class_vectors = positive_class_df.select(\n",
    "    positive_class_df.review_id,\n",
    "    list_to_vector_udf(positive_class_df.array).alias(\"vector\")\n",
    ")\n",
    "negative_class_vectors = negative_class_df.select(\n",
    "    negative_class_df.review_id,\n",
    "    list_to_vector_udf(negative_class_df.array).alias(\"vector\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41a054c790341279bda48c35977c0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# using PCA to reduce the dimension\n",
    "# for positive and negative class reduce the dimension to 125 which contains 95% information\n",
    "pca = PCA(k=125, inputCol=\"vector\", outputCol=\"pca\")\n",
    "pca_result = lambda vectors : pca.fit(vectors).transform(vectors).select('pca')\n",
    "positive_pca_result = pca_result(positive_class_vectors) \\\n",
    "                        .withColumn('index',f.monotonically_increasing_id())  \n",
    "negative_pca_result = pca_result(negative_class_vectors) \\\n",
    "                        .withColumn('index',f.monotonically_increasing_id())\n",
    "#delete the variables to recycle the memory\n",
    "del positive_class_vectors\n",
    "del negative_class_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc19df94f584150a3e5e75fb257a274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#broadcast points to all nodes\n",
    "positive_pca_broadcast = positive_pca_result.rdd.map(lambda item: (item[1],np.array(item[0]))).collect()\n",
    "positive_pca_broadcast =sc.broadcast(positive_pca_broadcast)\n",
    "negative_pca_broadcast = negative_pca_result.rdd.map(lambda item: (item[1],np.array(item[0]))).collect()\n",
    "negative_pca_broadcast =sc.broadcast(negative_pca_broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e7857800d24133bec78cb16d3722ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#calculating the cosine distance\n",
    "def positive_cosine_dis(item):\n",
    "    dis_list = []\n",
    "    pca_bdt = positive_pca_broadcast.value\n",
    "    cos = lambda vector1,vector2 : np.dot(vector1,vector2)/(np.linalg.norm(vector1)*(np.linalg.norm(vector2)))\n",
    "    for i in range(item[0]+1,len(pca_bdt)):\n",
    "        dis_list.append([pca_bdt[i][0],1-cos(item[1],pca_bdt[i][1])])\n",
    "    return [[(item[0],i[0]), i[1].tolist()] for i in dis_list]\n",
    "def negative_cosine_dis(item):\n",
    "    dis_list = []\n",
    "    pca_bdt = negative_pca_broadcast.value\n",
    "    cos = lambda vector1,vector2 : np.dot(vector1,vector2)/(np.linalg.norm(vector1)*(np.linalg.norm(vector2)))\n",
    "    for i in range(item[0]+1,len(pca_bdt)):\n",
    "        dis_list.append([pca_bdt[i][0],1-cos(item[1],pca_bdt[i][1])])\n",
    "    return [[(item[0],i[0]), i[1].tolist()] for i in dis_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de030e057e70487db78f7d90e4792a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define distance df for each class\n",
    "positive_distance_rdd = positive_pca_result.rdd.repartition(120) \\\n",
    "    .map(lambda item: (item[1],np.array(item[0]))) \\\n",
    "    .flatMap(positive_cosine_dis) \\\n",
    "    .cache()\n",
    "positive_distance_df = spark.createDataFrame(positive_distance_rdd, ['point','distance'])\n",
    "negative_distance_rdd = negative_pca_result.rdd.repartition(120) \\\n",
    "    .map(lambda item: (item[1],np.array(item[0]))) \\\n",
    "    .flatMap(negative_cosine_dis) \\\n",
    "    .cache()\n",
    "negative_distance_df = spark.createDataFrame(negative_distance_rdd, ['point','distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a39ecc3a6f14579a0c5f97f6f0428aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(distance)|\n",
      "+------------------+\n",
      "|0.6807390314585274|\n",
      "+------------------+\n",
      "\n",
      "+------------------+\n",
      "|     avg(distance)|\n",
      "+------------------+\n",
      "|0.7035416887820123|\n",
      "+------------------+"
     ]
    }
   ],
   "source": [
    "#calculate mean for the distance for each class\n",
    "avg_positive_distance = positive_distance_df.select(mean(positive_distance_df.distance)).show()\n",
    "avg_negative_distance = negative_distance_df.select(mean(negative_distance_df.distance)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e1aba34b31452bb67cf6a14445c6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create the index -> distance_list df, the distance_list contains all points and distance it connects\n",
    "def transform_distance_rdd(df):\n",
    "    '''\n",
    "    combine the key with connected all points and distance to form the new rdd \n",
    "    '''\n",
    "    trans_rdd = df.rdd.flatMap(lambda item : [[item[0][0],(item[0][1],item[1])],[item[0][1],(item[0][0],item[1])]]) \\\n",
    "                                .mapValues(lambda val : [val]) \\\n",
    "                                .reduceByKey(lambda x,y: x+y)\n",
    "    return trans_rdd\n",
    "negative_distance_list_df = spark.createDataFrame(transform_distance_rdd(negative_distance_df),['index','distance_list'])\n",
    "positive_distance_list_df = spark.createDataFrame(transform_distance_rdd(positive_distance_df),['index','distance_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa0a1b04faa4f64b93c360dbf553fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def min_avg_distance(df):\n",
    "    '''\n",
    "    calculate the minimum avg distance for each point\n",
    "    return the index of the minimum row that is the class centre\n",
    "    '''\n",
    "    avg_distance_udf = udf(lambda distance_list: float(np.mean(np.array(distance_list)[:,1],axis=0)), DoubleType())\n",
    "    return df.withColumn('avg_distance',avg_distance_udf(df.distance_list)) \\\n",
    "                        .drop('distance_list') \\\n",
    "                        .rdd \\\n",
    "                        .reduce(lambda x,y : (x[0],x[1]) if (x[1] < y[1]) else (y[0],y[1]))[0]\n",
    "positive_centre_index = min_avg_distance(positive_distance_list_df)\n",
    "negative_centre_index = min_avg_distance(negative_distance_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1cefeff129466b82c317dfdb0759bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_neighbors_index(index,df,class_df):\n",
    "    '''\n",
    "    input: index of the class centre, df\n",
    "    output: the 10 nearest neighbors list of the index\n",
    "    \n",
    "    '''\n",
    "    rdd = df.filter(df.index == index).select('distance_list') \\\n",
    "            .rdd.flatMap(lambda data: [[item[0],item[1]]for item in data.distance_list])\n",
    "    index_list = spark.createDataFrame(rdd,['index','distance']) \\\n",
    "                        .orderBy(f.asc('distance')) \\\n",
    "                        .limit(10) \\\n",
    "                        .select('index') \\\n",
    "                        .collect()\n",
    "    index_list = [item['index'] for item in index_list] #top 10 nearest neighbors index list\n",
    "    return index_list\n",
    "positive_neighbors_index = get_neighbors_index(positive_centre_index,positive_distance_list_df,positive_class_df)\n",
    "negative_neighbors_index = get_neighbors_index(negative_centre_index,negative_distance_list_df,negative_class_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c03282d9ae4c1d835e8ecd9dbe11f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+\n",
      "|review_id     |sentence              |\n",
      "+--------------+----------------------+\n",
      "|R32NXZWC3RKH76|Every song is awesome.|\n",
      "+--------------+----------------------+\n",
      "\n",
      "+--------------+---------------------------------------------------------------------------+\n",
      "|review_id     |sentence                                                                   |\n",
      "+--------------+---------------------------------------------------------------------------+\n",
      "|R2L4PZC7CHGQ4R|This album is much too insipid for people who like to listen to real music.|\n",
      "+--------------+---------------------------------------------------------------------------+\n",
      "\n",
      "+--------------+-----------------------------------------------+\n",
      "|review_id     |sentence                                       |\n",
      "+--------------+-----------------------------------------------+\n",
      "|R2L30H6HKZXUK7|every song is really good.                     |\n",
      "|R144NZND4C5S5A|Every single song is GREAT!                    |\n",
      "|R1VR2BOC4IT29H|Every song is great!                           |\n",
      "|R2XN70XMURMD4B|!Every single song on it is awesome!           |\n",
      "|R2N0NUDL7GEOCN|i LOVE every single song.                      |\n",
      "|RQUTYHT559L64 |The lyrics are awesome and every song is great.|\n",
      "|R3H0FC5EB40WT9|i like every song.                             |\n",
      "|R27M6LF1SF31D3|Every song is amazing.                         |\n",
      "|R3CR46JJVYB5W9|Every song on it is absolutely awesome.        |\n",
      "|R22FO8P9ERJA5M|all the songs are awesome!                     |\n",
      "+--------------+-----------------------------------------------+\n",
      "\n",
      "+--------------+-----------------------------------------------------------------------------------------------+\n",
      "|review_id     |sentence                                                                                       |\n",
      "+--------------+-----------------------------------------------------------------------------------------------+\n",
      "|R1CA8OIEZ0B22Y|I do like Blink 182 but most of this type of this music is junk.                               |\n",
      "|R2UBEJ5JX4PKX1|Almost everything that is wrong with mainstream music today, is represented in this album.     |\n",
      "|R195DYX83KWYXU|Simply put, this album is mediocre at best.                                                    |\n",
      "|R1DP63VJ4NXY44|This album absolutely sucks.                                                                   |\n",
      "|R1DP63VJ4NXY44|This album is full of teeny-bopper pop songs disguised as rock.                                |\n",
      "|RKN17VFTZQ69P |This album is horrid.                                                                          |\n",
      "|R36BAXRVCR2PFB|I bet most people who like this album don't even like real punk.                               |\n",
      "|RIQZE81DD7ZM7 |While I haven't heard all of the album, this is the worst band I've heard in a long, long time.|\n",
      "|R3QG1GKJO8IL4K|This album has in no way let down that genre.                                                  |\n",
      "|RATB9UCW9ZV0B |This album is a waste of time and money.                                                       |\n",
      "+--------------+-----------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "#class centre sentence and its 10 nearest neighbors\n",
    "def show_centre_sentence(df,index):\n",
    "    df.where(df.index == index).select('review_id','sentence').show(1,False)\n",
    "def show_neighbors(df,index_list):\n",
    "    df.where(df.index.isin(index_list)).select('review_id','sentence').show(10,False)\n",
    "show_centre_sentence(positive_class_df,positive_centre_index)\n",
    "show_centre_sentence(negative_class_df,negative_centre_index)\n",
    "show_neighbors(positive_class_df,positive_neighbors_index)\n",
    "show_neighbors(negative_class_df,negative_neighbors_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
